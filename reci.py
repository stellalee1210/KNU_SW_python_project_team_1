import requests
from bs4 import BeautifulSoup
import csv
import time
import os
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# âœ… ì—¬ê¸°ë§Œ ë°”ê¾¸ë©´ ì›í•˜ëŠ” êµ¬ê°„ë§Œ í¬ë¡¤ë§ ê°€ëŠ¥
start_page = 2292  # ì‹œì‘ í˜ì´ì§€
end_page = 6186    # ë í˜ì´ì§€

# ì„¸ì…˜ ì„¤ì •
session = requests.Session()
retry = Retry(connect=5, backoff_factor=1)
adapter = HTTPAdapter(max_retries=retry)
session.mount("http://", adapter)
session.mount("https://", adapter)

base_url = "https://www.10000recipe.com"
headers = {"User-Agent": "Mozilla/5.0"}

csv_filename = "ì „ì²´_ë ˆì‹œí”¼_ì´ë¯¸ì§€í¬í•¨.csv"
fieldnames = ["ì œëª©", "ëŒ€í‘œ ì´ë¯¸ì§€ URL", "ì¬ë£Œ", "ì–‘ë…", "ì¡°ë¦¬ ìˆœì„œ"]

# ê¸°ì¡´ ì €ì¥ ì œëª© ë¶ˆëŸ¬ì˜¤ê¸°
existing_titles = set()
if os.path.exists(csv_filename):
    with open(csv_filename, encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        for row in reader:
            existing_titles.add(row["ì œëª©"])
    print(f"ğŸ“„ ì €ì¥ëœ ë ˆì‹œí”¼: {len(existing_titles)}ê°œ")

# ìƒˆ íŒŒì¼ì¼ ê²½ìš° í—¤ë” ì‘ì„±
if not os.path.exists(csv_filename):
    with open(csv_filename, mode="w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()

count = len(existing_titles)

def clean_items(ul_tag):
    items = []
    for li in ul_tag.select("li"):
        lines = li.get_text(separator="\n").strip().split("\n")
        clean_lines = [line.strip() for line in lines if "êµ¬ë§¤" not in line and line.strip()]
        if len(clean_lines) >= 2:
            items.append(f"{clean_lines[0]}\n{clean_lines[1]}")
        elif len(clean_lines) == 1:
            items.append(clean_lines[0])
    return items

# âœ… ì§€ì •ëœ ë²”ìœ„ë§Œ í¬ë¡¤ë§
for page in range(start_page, end_page + 1):
    print(f"ğŸŒ í˜ì´ì§€ {page}/{end_page} ì²˜ë¦¬ ì¤‘...")

    try:
        list_url = f"{base_url}/recipe/list.html?order=reco&page={page}"
        res = session.get(list_url, headers=headers)
        soup = BeautifulSoup(res.text, "html.parser")
        cards = soup.select("ul.common_sp_list_ul li.common_sp_list_li a.common_sp_link")

        for card in cards:
            detail_url = base_url + card["href"]

            try:
                detail_res = session.get(detail_url, headers=headers)
                detail_soup = BeautifulSoup(detail_res.text, "html.parser")

                # ì œëª©
                title_tag = detail_soup.select_one("div.view2_summary h3")
                title = title_tag.text.strip() if title_tag else "ì œëª© ì—†ìŒ"
                if title in existing_titles: 
                    print(f"â© ì´ë¯¸ ì €ì¥ë¨: {title}")
                    continue

                # ëŒ€í‘œ ì´ë¯¸ì§€ URL
                image_tag = detail_soup.select_one("div.centeredcrop img")
                image_url = image_tag["src"] if image_tag else ""

                # ì¬ë£Œ / ì–‘ë…
                ingre_lists = detail_soup.select("div.ready_ingre3 ul")
                ingredient_text = ""
                seasoning_text = ""

                if len(ingre_lists) >= 1:
                    ingredient_text = "\n".join(clean_items(ingre_lists[0]))
                if len(ingre_lists) >= 2:
                    seasoning_text = "\n".join(clean_items(ingre_lists[1]))

                # ì¡°ë¦¬ ìˆœì„œ (ë§í¬ë§Œ ì œê±°)
                step_tags = detail_soup.select("div.view_step div.view_step_cont")
                steps = []
                for s in step_tags:
                    for a_tag in s.find_all("a"):
                        a_tag.decompose()
                    plain = s.get_text(separator=" ").strip()
                    if "êµ¬ë§¤" not in plain and plain:
                        steps.append(plain)
                steps_text = "\n".join([f"{i+1}. {s}" for i, s in enumerate(steps)])

                # ì €ì¥
                with open(csv_filename, mode="a", newline="", encoding="utf-8-sig") as f:
                    writer = csv.DictWriter(f, fieldnames=fieldnames)
                    writer.writerow({
                        "ì œëª©": title,
                        "ëŒ€í‘œ ì´ë¯¸ì§€ URL": image_url,
                        "ì¬ë£Œ": ingredient_text,
                        "ì–‘ë…": seasoning_text,
                        "ì¡°ë¦¬ ìˆœì„œ": steps_text
                    })

                existing_titles.add(title)
                count += 1
                print(f"âœ… ì €ì¥ ì™„ë£Œ ({count}) : {title}")

            except Exception as e:
                print(f"âš ï¸ ìƒì„¸ í˜ì´ì§€ ì—ëŸ¬: {e}")
            time.sleep(1)

    except Exception as e:
        print(f"âŒ ëª©ë¡ í˜ì´ì§€ ì—ëŸ¬ (page {page}): {e}")

    if page % 100 == 0:
        print("ğŸ›Œ 100í˜ì´ì§€ ë§ˆë‹¤ 10ì´ˆ íœ´ì‹...")
        time.sleep(10)

print(f"ğŸ‰ ì§€ì • ë²”ìœ„ í¬ë¡¤ë§ ì™„ë£Œ! ì´ {count}ê°œ ì €ì¥ë¨ âœ…")
